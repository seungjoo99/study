{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRSvB_DWqKRe"
   },
   "source": [
    "# Diabetic Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JH2sFBQAqKRh"
   },
   "outputs": [],
   "source": [
    "# #First Do This.\n",
    "# !kaggle competitions download -c diabetic-retinopathy-detection\n",
    "\n",
    "# !unzip diabetic-retinopathy-detection.zip\n",
    "\n",
    "# !unzip sampleSubmission.csv.zip\n",
    "# !unzip trainLabels.csv.zip\n",
    "\n",
    "# !cat test.zip* > ./test.zip\n",
    "# !unzip test.zip\n",
    "\n",
    "# !cat train.zip* > ./train.zip\n",
    "# !unzip train.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XrNDQmcFqKRr"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "img = mpimg.imread('./train_preprocess/10_left.jpeg')\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "img = mpimg.imread('./train_preprocess/10_right.jpeg')\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "img = mpimg.imread('./train_preprocess/13_left.jpeg')\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "img = mpimg.imread('./train_preprocess/13_right.jpeg')\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oEXOyJC-qKSX"
   },
   "source": [
    "## 문제점\n",
    "1. 이미지의 크기가 다르다\n",
    "2. 몇몇의 이미지는 음영이 고르지 않다.\n",
    "3. 몇몇의 이미지는 cup과 disc의 위치가 중앙에 잡혀있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJ_AqRqCqKSY"
   },
   "source": [
    "## Image preprocess\n",
    "1. Left image flip => 좌안의 이미지를 반전시킨다.\n",
    "2. Image histogram equalization => 이미지 평활화를 통해 음영을 고르게 한다.\n",
    "3. Image resize => 이미지의 크기가 제각각이므로 resize한다.\n",
    "4. (Optional) ROI cut process => 만약 해당 도메인에서 관심 영역이 정해져있다면 해당 관심영역만을 절단하여 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80zQ0fUmqKSZ"
   },
   "source": [
    "### Image resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wy2qIIFMqKSb"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "from shutil import copyfile\n",
    "\n",
    "# Get filenames\n",
    "image_files_list = list(glob.glob('train/*'))\n",
    "\n",
    "# Directory check\n",
    "new_dir = 'train_preprocess/'\n",
    "if not os.path.isdir(new_dir):\n",
    "    os.mkdir(new_dir)\n",
    "\n",
    "# OS image resize\n",
    "for image_file in image_files_list:\n",
    "    im = Image.open(image_file)\n",
    "    im = im.resize((448, 448))\n",
    "    new_image_path = new_dir + image_file.split(\"/\")[-1]\n",
    "    im.save(new_image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLJL1P3JqKSh"
   },
   "source": [
    "### Left image flip process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LL9xcQxyqKSi"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "# Get filenames\n",
    "image_files_list = list(glob.glob('train_preprocess/*'))\n",
    "\n",
    "# OS image flip algorithm\n",
    "for image_file in image_files_list:\n",
    "    if image_file.split(\"/\")[-1].split(\"_\")[1].split(\".\")[0] == \"left\":\n",
    "        im = Image.open(image_file)\n",
    "        im = im.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        im.save(image_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LdduPoxqKSn"
   },
   "source": [
    "### Image histogram equalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A-pdCQVZqKSo"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import cv2\n",
    "\n",
    "# Get filenames\n",
    "image_files_list = list(glob.glob('train_preprocess/*'))\n",
    "\n",
    "# OS image flip algorithm\n",
    "for image_file in image_files_list:\n",
    "    # OpenCV의 Equaliztion함수\n",
    "    image = cv2.imread(image_file)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2YUV)\n",
    "    image[:,:,0] = cv2.equalizeHist(image[:,:,0])\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_YUV2BGR)\n",
    "    cv2.imwrite(image_file, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QJdN_ml3qKSu"
   },
   "outputs": [],
   "source": [
    "!zip train_preprocess.zip ./train_preprocess/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_Kjp7CXqKS-"
   },
   "source": [
    "## Model pipeline\n",
    "1. csv(data frame)을 기준으로 image를 차례차례 불러오는 data_generator를 만들기\n",
    "2. image plotting을 통해 image generator 결과 보이기\n",
    "3. 모델 선언 및 학습하기\n",
    "4. 결과 보이기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GaVFzfAhqKS_"
   },
   "source": [
    "### 1. train_df sampling\n",
    "(1) train_df를 2번 label에 맞춰 각각 under & over sampling을 진행함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "drcpZxxeqKTA"
   },
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv(\"trainLabels.csv\")\n",
    "\n",
    "# df_class_0 = train_df[train_df['level'] == 0]\n",
    "# df_class_1 = train_df[train_df['level'] == 1]\n",
    "# df_class_2 = train_df[train_df['level'] == 2]\n",
    "# df_class_3 = train_df[train_df['level'] == 3]\n",
    "# df_class_4 = train_df[train_df['level'] == 4]\n",
    "\n",
    "# print(train_df.level.value_counts())\n",
    "# count_class = train_df.level.value_counts()[2]\n",
    "\n",
    "# df_class_0_under = df_class_0.sample(count_class)\n",
    "# df_class_1_over = df_class_1.sample(count_class, replace=True)\n",
    "# df_class_3_over = df_class_3.sample(count_class, replace=True)\n",
    "# df_class_4_over = df_class_4.sample(count_class, replace=True)\n",
    "# train_df = pd.concat([df_class_0_under, df_class_1_over, df_class_2, df_class_3_over, df_class_4_over], axis=0)\n",
    "\n",
    "# train_df['level'].hist(figsize = (10, 5))\n",
    "\n",
    "# train_df['image']= train_df['image'] + '.jpeg'\n",
    "# train_df = shuffle(train_df)\n",
    "# train_df.to_csv('train_df.csv', sep=',', na_rep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BbQFzQR4qKTF"
   },
   "source": [
    "(2) train_df를 정상 label에 맞춰 각각 over sampling을 진행함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IKjEHQYrqKTG"
   },
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv(\"trainLabels.csv\")\n",
    "\n",
    "# df_class_0 = train_df[train_df['level'] == 0]\n",
    "# df_class_1 = train_df[train_df['level'] == 1]\n",
    "# df_class_2 = train_df[train_df['level'] == 2]\n",
    "# df_class_3 = train_df[train_df['level'] == 3]\n",
    "# df_class_4 = train_df[train_df['level'] == 4]\n",
    "\n",
    "# print(train_df.level.value_counts())\n",
    "# count_class = train_df.level.value_counts()[0]\n",
    "\n",
    "# df_class_1_over = df_class_1.sample(count_class, replace=True)\n",
    "# df_class_2_over = df_class_2.sample(count_class, replace=True)\n",
    "# df_class_3_over = df_class_3.sample(count_class, replace=True)\n",
    "# df_class_4_over = df_class_4.sample(count_class, replace=True)\n",
    "# train_df = pd.concat([df_class_0, df_class_1_over, df_class_2_over, df_class_3_over, df_class_4_over], axis=0)\n",
    "\n",
    "# train_df['level'].hist(figsize = (10, 5))\n",
    "\n",
    "# train_df['image']= train_df['image'] + '.jpeg'\n",
    "# train_df = shuffle(train_df)\n",
    "# train_df.to_csv('train_df.csv', sep=',', na_rep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w97x48QXqKTL"
   },
   "source": [
    "### 2. Deep learning 진행하기\n",
    "(1) 성능 향상시키기\n",
    "(2) 딥러닝에서 발생하는 여러 문제 해결하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "W2EKwngXqKTM"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os, datetime\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "id": "zeuj4Y4MqKTR",
    "outputId": "296ef299-cb04-4e11-b14d-816ff9f276f9"
   },
   "outputs": [],
   "source": [
    "# memory 관련 이슈를 해결하기 위한 코드\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "id": "HyIcjuMdqKTX",
    "outputId": "acce117e-931e-49b9-e728-e29ceba44f09"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'train_df.csv' does not exist: b'train_df.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-053fade3835f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 위의 작업들로 생성된 train_df을 불러옴\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"train_df.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'level'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'level'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'train_df.csv' does not exist: b'train_df.csv'"
     ]
    }
   ],
   "source": [
    "# 위의 작업들로 생성된 train_df을 불러옴\n",
    "train_df = pd.read_csv(\"train_df.csv\")\n",
    "train_df\n",
    "train_df['level']= train_df['level'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FaAvY7d9qKTj"
   },
   "outputs": [],
   "source": [
    "# 사전 설정을 위한 세팅\n",
    "epochs = 128\n",
    "image_size = 448\n",
    "batch = 64\n",
    "\n",
    "# Directory check\n",
    "new_dir = 'ckpt/'\n",
    "if not os.path.isdir(new_dir):\n",
    "    os.mkdir(new_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CNAaXJwTqKTn"
   },
   "outputs": [],
   "source": [
    "train_image_generator = ImageDataGenerator(rescale=1./255, validation_split=0.2) # Generator for our training, validation data\n",
    "test_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jeHlYlY5qKTs"
   },
   "outputs": [],
   "source": [
    "train_data_gen = train_image_generator.flow_from_dataframe(dataframe=train_df,\n",
    "                                                           directory=\"./train_preprocess\",\n",
    "                                                           x_col=\"image\",\n",
    "                                                           y_col=\"level\",\n",
    "                                                           class_mode=\"categorical\",\n",
    "                                                           shuffle=True,\n",
    "                                                           target_size=(image_size,image_size),\n",
    "                                                           batch_size=batch,\n",
    "                                                           subset='training')\n",
    "\n",
    "valid_data_gen = train_image_generator.flow_from_dataframe(dataframe=train_df,\n",
    "                                                           directory=\"./train_preprocess\",\n",
    "                                                           x_col=\"image\",\n",
    "                                                           y_col=\"level\",\n",
    "                                                           class_mode=\"categorical\",\n",
    "                                                           shuffle=True,\n",
    "                                                           target_size=(image_size,image_size),\n",
    "                                                           batch_size=batch,\n",
    "                                                           subset='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h9FAfOX9qKTx"
   },
   "outputs": [],
   "source": [
    "sample_training_images, _ = next(train_data_gen)\n",
    "\n",
    "# This function will plot images in the form of a grid with 1 row and 5 columns where images are placed in each column.\n",
    "def plotImages(images_arr):\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(20,20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip( images_arr, axes):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "plotImages(sample_training_images[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ug5VXJFvqKT2"
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(8, 3, padding='same', activation='relu',\n",
    "           input_shape=(image_size, image_size ,3)),\n",
    "    Dropout(0.5),\n",
    "    Conv2D(16, 1, strides=2, padding='same', activation='relu'),\n",
    "    Conv2D(16, 3, padding='same', activation='relu'),\n",
    "    Conv2D(16, 1, padding='same', activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Conv2D(8, 1, strides=2, padding='same', activation='relu'),\n",
    "#     Conv2D(8, 3, padding='same', activation='relu', activity_regularizer=l1_l2(l1=5e-6, l2=5e-6)),\n",
    "    Conv2D(8, 3, padding='same', activation='relu'),\n",
    "    Conv2D(8, 1, padding='same', activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Flatten(),\n",
    "    Dense(128, activation=\"relu\", activity_regularizer=l1_l2(l1=2e-5, l2=2e-5)),\n",
    "    Dense(5, activation=\"softmax\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 652
    },
    "id": "hCtDKJa4qKT8",
    "outputId": "a1fe5281-e018-420c-93d8-aeaabbd853d6",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 448, 448, 8)       224       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 448, 448, 8)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 16)      144       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 224, 224, 16)      2320      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 224, 224, 16)      272       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 224, 224, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 112, 112, 8)       136       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 112, 112, 8)       584       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 112, 112, 8)       72        \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 112, 112, 8)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 100352)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               12845184  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 12,849,581\n",
      "Trainable params: 12,849,581\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vi_EIcwEqKUS"
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        # Path where to save the model\n",
    "        # The two parameters below mean that we will overwrite\n",
    "        # the current checkpoint if and only if\n",
    "        # the `val_loss` score has improved.\n",
    "        # The saved model name will include the current epoch.\n",
    "        filepath=\"ckpt/mymodel_best.h5\",\n",
    "        save_best_only=True,  # Only save a model if `val_loss` has improved.\n",
    "        monitor=\"val_loss\",\n",
    "        verbose=1,\n",
    "    ),\n",
    "#     EarlyStopping(\n",
    "#         # Stop training when `val_loss` is no longer improving\n",
    "#         monitor=\"val_loss\",\n",
    "#         # \"no longer improving\" being defined as \"no better than 1e-2 less\"\n",
    "#         min_delta=1e-4,\n",
    "#         # \"no longer improving\" being further defined as \"for at least 2 epochs\"\n",
    "#         patience=10,\n",
    "#         verbose=1,\n",
    "#     ),\n",
    "    TensorBoard(logdir, histogram_freq=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PAPOzD2uqKUW",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_data_gen,\n",
    "    epochs=epochs,\n",
    "    validation_data=valid_data_gen,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "edcqC587qKUa",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(len(acc)), acc, label='Training Accuracy')\n",
    "plt.plot(range(len(val_acc)), val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(len(loss)), loss, label='Training Loss')\n",
    "plt.plot(range(len(val_loss)), val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dsKc5A76qKUe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LyC9ojNpqKUi"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "diabetic_v.2.0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
